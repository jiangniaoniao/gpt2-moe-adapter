import torch
import torch.nn as nn
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import os
import json
import numpy as np

class SmearTrainer:
    """SMEARè®­ç»ƒå™¨ - ä¿®å¤æ—©åœæœºåˆ¶"""
    
    def __init__(self, model, train_loader, val_loader, test_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.config = config
        
        # ä¼˜åŒ–å™¨ - åªè®­ç»ƒAdapterå‚æ•°
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        self.optimizer = AdamW(trainable_params, lr=config.learning_rate, weight_decay=config.weight_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=config.warmup_steps,
            num_training_steps=config.total_steps
        )
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_stats = {
            'losses': [],
            'perplexities': [],
            'learning_rates': [],
            'test_loss': None,
            'test_perplexity': None,
            'best_val_loss': float('inf'),
            'best_epoch': -1,
            'early_stop_epoch': None
        }
        
        # æ—©åœç›¸å…³å˜é‡
        self.patience = getattr(config, 'patience', 3)  # é»˜è®¤å®¹å¿3ä¸ªepochæ²¡æœ‰æ”¹å–„
        self.patience_counter = 0
        self.min_delta = getattr(config, 'min_delta', 1e-4)  # æœ€å°æ”¹å–„é˜ˆå€¼
        
        print(f"ğŸš€ åˆå§‹åŒ–SMEARè®­ç»ƒå™¨")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,}")
        print(f"   - æ—©åœè€å¿ƒå€¼: {self.patience} epochs")
        print(f"   - æœ€å°æ”¹å–„é˜ˆå€¼: {self.min_delta}")
        if test_loader is not None:
            print(f"   - æµ‹è¯•é›†å¤§å°: {len(test_loader.dataset)} æ ·æœ¬")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        
        for batch_idx, batch in enumerate(progress_bar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs['loss']
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
            
            self.optimizer.step()
            self.scheduler.step()
            
            # åªè®°å½•æ ¸å¿ƒæŸå¤±
            total_loss += loss.item()
            
            # ç®€åŒ–çš„è¿›åº¦æ¡ - åªæ˜¾ç¤ºæ ¸å¿ƒæŒ‡æ ‡
            current_lr = self.scheduler.get_last_lr()[0]
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'LR': f'{current_lr:.2e}',
                'Patience': f'{self.patience_counter}/{self.patience}'
            })
            
            # æ¯100ä¸ªbatchè®°å½•ä¸€æ¬¡å­¦ä¹ ç‡ï¼ˆå¯é€‰ï¼‰
            if batch_idx % 100 == 0:
                self.train_stats['learning_rates'].append(current_lr)
        
        # è®°å½•epochç»Ÿè®¡
        avg_loss = total_loss / len(self.train_loader)
        self.train_stats['losses'].append(avg_loss)
        
        return avg_loss
    
    def validate(self, epoch):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        total_perplexity = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs['loss']
                total_loss += loss.item()
                
                # è®¡ç®—å›°æƒ‘åº¦
                perplexity = torch.exp(torch.tensor(loss.item()))
                total_perplexity += perplexity.item()
        
        avg_loss = total_loss / len(self.val_loader)
        avg_perplexity = total_perplexity / len(self.val_loader)
        
        self.train_stats['perplexities'].append(avg_perplexity)
        
        return avg_loss, avg_perplexity
    
    def test(self, model_path=None):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        if self.test_loader is None:
            print("âš ï¸  æœªæä¾›æµ‹è¯•é›†ï¼Œè·³è¿‡æµ‹è¯•è¯„ä¼°")
            return None, None
        
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹è·¯å¾„ï¼Œåˆ™é‡æ–°åŠ è½½å®Œæ•´æ¨¡å‹
        if model_path is not None:
            self.load_complete_model(model_path)
            print(f"ğŸ“‚ åŠ è½½å®Œæ•´æ¨¡å‹è¿›è¡Œæµ‹è¯•: {model_path}")
        
        self.model.eval()
        total_loss = 0
        total_perplexity = 0
        
        print("ğŸ§ª å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
        
        with torch.no_grad():
            for batch in tqdm(self.test_loader, desc='Testing'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs['loss']
                total_loss += loss.item()
                
                # è®¡ç®—å›°æƒ‘åº¦
                perplexity = torch.exp(torch.tensor(loss.item()))
                total_perplexity += perplexity.item()
        
        avg_loss = total_loss / len(self.test_loader)
        avg_perplexity = total_perplexity / len(self.test_loader)
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self.train_stats['test_loss'] = avg_loss
        self.train_stats['test_perplexity'] = avg_perplexity
        
        print(f"ğŸ¯ æµ‹è¯•é›†ç»“æœ:")
        print(f"  - æµ‹è¯•æŸå¤±: {avg_loss:.4f}")
        print(f"  - æµ‹è¯•å›°æƒ‘åº¦: {avg_perplexity:.4f}")
        
        return avg_loss, avg_perplexity
    
    def save_complete_model(self, path):
        """ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹ + SMEARé€‚é…å™¨ï¼‰"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        save_path = os.path.join(self.config.output_dir, path)
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´æ¨¡å‹çŠ¶æ€
        complete_state_dict = {
            'model_state_dict': self.model.state_dict(),
            'config': self.config.__dict__,
            'training_stats': self.train_stats,
            'smear_adapters_only': False  # æ ‡è®°è¿™æ˜¯å®Œæ•´æ¨¡å‹
        }
        
        torch.save(complete_state_dict, os.path.join(save_path, 'complete_model.pth'))
        print(f"ğŸ’¾ ä¿å­˜å®Œæ•´æ¨¡å‹åˆ° {save_path}")
    
    def save_smear_adapters_only(self, path):
        """ä»…ä¿å­˜SMEARé€‚é…å™¨å‚æ•°ï¼ˆç”¨äºç»§ç»­è®­ç»ƒï¼‰"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        save_path = os.path.join(self.config.output_dir, path)
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜SMEARé€‚é…å™¨å‚æ•°
        model_state_dict = self.model.state_dict()
        smear_state_dict = {
            name: param for name, param in model_state_dict.items()
            if any(key in name for key in ['smear_adapters', 'adapter_alpha'])
        }
        
        adapter_only_state_dict = {
            'smear_adapters': smear_state_dict,
            'config': self.config.__dict__,
            'training_stats': self.train_stats,
            'smear_adapters_only': True  # æ ‡è®°è¿™æ˜¯ä»…é€‚é…å™¨
        }
        
        torch.save(adapter_only_state_dict, os.path.join(save_path, 'smear_adapters.pth'))
        print(f"ğŸ’¾ ä¿å­˜ {len(smear_state_dict)} ä¸ªSMEARé€‚é…å™¨å‚æ•°åˆ° {save_path}")
    
    def load_complete_model(self, model_path):
        """åŠ è½½å®Œæ•´æ¨¡å‹"""
        checkpoint_path = os.path.join(model_path, 'complete_model.pth')
        
        if not os.path.exists(checkpoint_path):
            print(f"âŒ å®Œæ•´æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            print("âš ï¸  å°è¯•åŠ è½½ä»…é€‚é…å™¨ç‰ˆæœ¬...")
            return self.load_smear_adapters_only(model_path)
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # åŠ è½½å®Œæ•´æ¨¡å‹çŠ¶æ€
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # æ›´æ–°é…ç½®å’Œè®­ç»ƒç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
        if 'training_stats' in checkpoint:
            self.train_stats.update(checkpoint['training_stats'])
        
        print(f"ğŸ“¥ ä» {model_path} åŠ è½½å®Œæ•´æ¨¡å‹")
        return True
    
    def load_smear_adapters_only(self, model_path):
        """ä»…åŠ è½½SMEARé€‚é…å™¨å‚æ•°"""
        checkpoint_path = os.path.join(model_path, 'smear_adapters.pth')
        
        if not os.path.exists(checkpoint_path):
            print(f"âŒ é€‚é…å™¨æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return False
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # è·å–å½“å‰æ¨¡å‹çŠ¶æ€å­—å…¸
        model_state_dict = self.model.state_dict()
        
        # åªæ›´æ–°SMEARç›¸å…³çš„å‚æ•°
        smear_adapters = checkpoint['smear_adapters']
        for name, param in smear_adapters.items():
            if name in model_state_dict:
                model_state_dict[name].copy_(param)
            else:
                print(f"âš ï¸  è·³è¿‡ä¸åŒ¹é…çš„å‚æ•°: {name}")
        
        # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        self.model.load_state_dict(model_state_dict)
        
        # æ›´æ–°è®­ç»ƒç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
        if 'training_stats' in checkpoint:
            self.train_stats.update(checkpoint['training_stats'])
        
        print(f"ğŸ“¥ ä» {model_path} åŠ è½½SMEARé€‚é…å™¨å‚æ•°")
        return True
    
    def check_early_stop(self, current_val_loss, best_val_loss, epoch):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ - ä¿®å¤ç‰ˆæœ¬"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—æ”¹å–„ï¼ˆè¶…è¿‡æœ€å°é˜ˆå€¼ï¼‰
        improvement = best_val_loss - current_val_loss
        
        if improvement > self.min_delta:
            # æœ‰æ˜¾è‘—æ”¹å–„ï¼Œé‡ç½®è®¡æ•°å™¨
            self.patience_counter = 0
            print(f"âœ… éªŒè¯æŸå¤±æ”¹å–„: {improvement:.6f} > {self.min_delta}")
            return False
        else:
            # æ²¡æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¢åŠ è®¡æ•°å™¨
            self.patience_counter += 1
            print(f"â³ éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œè€å¿ƒè®¡æ•°: {self.patience_counter}/{self.patience}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è€å¿ƒé™åˆ¶
            if self.patience_counter >= self.patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {self.patience} ä¸ªepochéªŒè¯æŸå¤±æœªæ”¹å–„")
                self.train_stats['early_stop_epoch'] = epoch
                return True
            
            return False
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹ - ä¿®å¤æ—©åœæœºåˆ¶"""
        best_val_loss = float('inf')
        best_epoch = -1
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒSMEARé€‚é…å™¨æ¨¡å‹")
        
        for epoch in range(self.config.num_epochs):
            print(f"\nğŸ“ Epoch {epoch + 1}/{self.config.num_epochs}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_perplexity = self.validate(epoch)
            
            # ç®€åŒ–çš„è®­ç»ƒç»Ÿè®¡è¾“å‡º
            print(f"ğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
            print(f"  - è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  - éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  - éªŒè¯å›°æƒ‘åº¦: {val_perplexity:.4f}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
            has_improvement = val_loss < best_val_loss - self.min_delta
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåªåœ¨æ€§èƒ½æå‡æ—¶ä¿å­˜ï¼‰
            if has_improvement:
                best_val_loss = val_loss
                best_epoch = epoch
                
                # ä¿å­˜å®Œæ•´æ¨¡å‹ç”¨äºæµ‹è¯•
                self.save_complete_model("best_smear_model")
                # åŒæ—¶ä¿å­˜é€‚é…å™¨å‚æ•°ç”¨äºç»§ç»­è®­ç»ƒ
                self.save_smear_adapters_only("best_smear_adapters")
                
                self.train_stats['best_val_loss'] = best_val_loss
                self.train_stats['best_epoch'] = best_epoch
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f}, Epoch: {epoch})")
            else:
                print(f"ğŸ“‰ éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œè·³è¿‡ä¿å­˜ (å½“å‰æœ€ä½³: {best_val_loss:.4f})")
            
            # æ£€æŸ¥æ—©åœæ¡ä»¶ - åªåœ¨æ²¡æœ‰æ”¹å–„æ—¶æ£€æŸ¥
            if not has_improvement and self.check_early_stop(val_loss, best_val_loss, epoch):
                print(f"â¹ï¸  è®­ç»ƒåœ¨ Epoch {epoch} æå‰åœæ­¢")
                break
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            self.save_training_stats()
        
        # è®­ç»ƒç»“æŸååœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹
        print(f"\n{'='*50}")
        print("ğŸ¯ è®­ç»ƒå®Œæˆï¼Œå¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")
        print(f"{'='*50}")
        
        test_loss, test_perplexity = self.test("best_smear_model")
        
        # æœ€ç»ˆæŠ¥å‘Š
        print(f"\n{'='*50}")
        print("ğŸ æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š:")
        print(f"{'='*50}")
        print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f} (Epoch {best_epoch})")
        print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒè½®æ•°: {len(self.train_stats['losses'])}")
        if self.train_stats['early_stop_epoch'] is not None:
            print(f"â¹ï¸  æ—©åœè§¦å‘äº: Epoch {self.train_stats['early_stop_epoch']}")
        if test_loss is not None:
            print(f"ğŸ¯ æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
            print(f"ğŸ¯ æµ‹è¯•é›†å›°æƒ‘åº¦: {test_perplexity:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        self.save_final_report(best_val_loss, best_epoch, test_loss, test_perplexity)
        
        return best_val_loss
    
    def save_training_stats(self):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        with open(os.path.join(self.config.output_dir, 'smear_training_stats.json'), 'w') as f:
            json.dump(self.train_stats, f, indent=2)
    
    def save_final_report(self, best_val_loss, best_epoch, test_loss, test_perplexity):
        """ä¿å­˜æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š"""
        report = {
            'training_summary': {
                'best_validation_loss': best_val_loss,
                'best_epoch': best_epoch,
                'test_loss': test_loss,
                'test_perplexity': test_perplexity,
                'total_training_epochs': len(self.train_stats['losses']),
                'early_stop_epoch': self.train_stats['early_stop_epoch'],
                'final_learning_rate': self.train_stats['learning_rates'][-1] if self.train_stats['learning_rates'] else 0,
                'patience_used': self.patience_counter
            },
            'model_info': {
                'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),
                'device': str(self.device)
            },
            'early_stop_config': {
                'patience': self.patience,
                'min_delta': self.min_delta
            },
            'config': self.config.__dict__
        }
        
        report_path = os.path.join(self.config.output_dir, 'final_training_report.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")