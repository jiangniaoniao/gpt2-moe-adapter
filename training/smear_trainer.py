import torch
import torch.nn as nn
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import os
import json
import numpy as np
from torch.cuda.amp import autocast, GradScaler

class LongBenchEvaluator:
    """LongBenchè¯„ä¼°å™¨ - ä¸“é—¨æµ‹è¯•é•¿æ–‡æœ¬ç†è§£èƒ½åŠ› + FP16æ”¯æŒ"""
    
    def __init__(self, model, tokenizer, device, use_fp16=True):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.use_fp16 = use_fp16 and torch.cuda.is_available()
        self.results = {}
        
        print(f"  LongBenchè¯„ä¼°å™¨åˆå§‹åŒ– - FP16: {'å¯ç”¨' if self.use_fp16 else 'ç¦ç”¨'}")

    def evaluate_single_task(self, task_name, dataset, max_samples=20):
        """è¯„ä¼°å•ä¸ªLongBenchä»»åŠ¡ - æ”¯æŒFP16"""
        print(f"  è¯„ä¼°ä»»åŠ¡: {task_name}")
        
        if max_samples and len(dataset) > max_samples:
            dataset = dataset.select(range(max_samples))
        
        task_results = {
            'exact_match': [],
            'rouge_scores': [],
            'perplexity': []
        }
        
        for i, sample in enumerate(tqdm(dataset, desc=f"è¯„ä¼° {task_name}")):
            try:
                # æ„å»ºè¾“å…¥
                context = sample.get('context', '')
                question = sample.get('input', '') or sample.get('question', '')
                ground_truth = sample.get('answers', [''])[0] if sample.get('answers') else sample.get('target', '')
                
                # æ ¹æ®ä»»åŠ¡ç±»å‹æ„å»ºæç¤º
                if 'qa' in task_name.lower():
                    prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n\næ–‡æ¡£ï¼š{context}\n\né—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š"
                elif 'summar' in task_name.lower():
                    prompt = f"ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆæ‘˜è¦ï¼š\n\n{context}\n\næ‘˜è¦ï¼š"
                else:
                    prompt = f"{context}\n\n{question}"
                
                # åˆ†è¯
                inputs = self.tokenizer(
                    prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=2048  # é™åˆ¶è¾“å…¥é•¿åº¦
                ).to(self.device)
                
                # ç”Ÿæˆ - æ”¯æŒFP16
                with torch.no_grad():
                    if self.use_fp16:
                        with autocast():
                            outputs = self.model.generate(
                                inputs.input_ids,
                                max_new_tokens=256,
                                do_sample=False,
                                pad_token_id=self.tokenizer.eos_token_id,
                                num_return_sequences=1
                            )
                    else:
                        outputs = self.model.generate(
                            inputs.input_ids,
                            max_new_tokens=256,
                            do_sample=False,
                            pad_token_id=self.tokenizer.eos_token_id,
                            num_return_sequences=1
                        )
                
                # è§£ç ç”Ÿæˆç»“æœ
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                prediction = generated_text[len(prompt):].strip()
                
                # è®¡ç®—ç²¾ç¡®åŒ¹é…
                exact_match = 1.0 if prediction.strip() == ground_truth.strip() else 0.0
                task_results['exact_match'].append(exact_match)
                
                # è®¡ç®—å›°æƒ‘åº¦ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
                if context:
                    perplexity = self.calculate_perplexity(context)
                    task_results['perplexity'].append(perplexity)
                
                # æ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„ç¤ºä¾‹
                if i < 2:
                    print(f"\n--- {task_name} æ ·æœ¬ {i} ---")
                    print(f"è¾“å…¥: {prompt[:200]}...")
                    print(f"é¢„æµ‹: {prediction[:100]}...")
                    print(f"çœŸå®: {ground_truth[:100]}...")
                    print(f"ç²¾ç¡®åŒ¹é…: {exact_match}")
                    print(f"FP16: {'å¯ç”¨' if self.use_fp16 else 'ç¦ç”¨'}")
                    
            except Exception as e:
                print(f"  è¯„ä¼°æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                continue
        
        # æ±‡æ€»ä»»åŠ¡ç»“æœ
        if task_results['exact_match']:
            task_summary = {
                'exact_match': np.mean(task_results['exact_match']),
                'samples_evaluated': len(task_results['exact_match']),
                'fp16_enabled': self.use_fp16
            }
            if task_results['perplexity']:
                task_summary['avg_perplexity'] = np.mean(task_results['perplexity'])
            
            self.results[task_name] = task_summary
            return task_summary
        else:
            print(f"  ä»»åŠ¡ {task_name} æ— æœ‰æ•ˆç»“æœ")
            return None

    def calculate_perplexity(self, text):
        """è®¡ç®—æ–‡æœ¬çš„å›°æƒ‘åº¦ - æ”¯æŒFP16"""
        try:
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=1024).to(self.device)
            with torch.no_grad():
                if self.use_fp16:
                    with autocast():
                        outputs = self.model(inputs.input_ids, labels=inputs.input_ids)
                        loss = outputs.loss
                else:
                    outputs = self.model(inputs.input_ids, labels=inputs.input_ids)
                    loss = outputs.loss
                
                perplexity = torch.exp(loss).item()
            return perplexity
        except:
            return float('inf')
    
    def evaluate_all_tasks(self, max_samples_per_task=20):
        """è¯„ä¼°æ‰€æœ‰LongBenchä»»åŠ¡"""
        print("  å¼€å§‹LongBenchä»»åŠ¡è¯„ä¼°")
        
        # åŠ è½½LongBenchæ•°æ®é›†
        try:
            longbench_tasks = {
                'single_doc_qa': load_dataset("THUDM/LongBench", "single_doc_qa", split="test"),
                'multi_doc_qa': load_dataset("THUDM/LongBench", "multi_doc_qa", split="test"),
                'summarization': load_dataset("THUDM/LongBench", "summarization", split="test")
            }
        except Exception as e:
            print(f"  åŠ è½½LongBenchå¤±è´¥: {e}")
            return self.results
        
        # è¯„ä¼°æ¯ä¸ªä»»åŠ¡
        for task_name, dataset in longbench_tasks.items():
            if dataset is not None:
                self.evaluate_single_task(task_name, dataset, max_samples_per_task)
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        self.generate_report()
        return self.results
    
    def generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        if not self.results:
            print("  æ— è¯„ä¼°ç»“æœå¯æŠ¥å‘Š")
            return
        
        print("\n" + "="*60)
        print("  LongBenchè¯„ä¼°æŠ¥å‘Š")
        print("="*60)
        
        overall_scores = []
        for task_name, scores in self.results.items():
            em_score = scores.get('exact_match', 0)
            overall_scores.append(em_score)
            print(f"  {task_name}:")
            print(f"   - ç²¾ç¡®åŒ¹é…: {em_score:.4f}")
            print(f"   - è¯„ä¼°æ ·æœ¬: {scores.get('samples_evaluated', 0)}")
            if 'avg_perplexity' in scores:
                print(f"   - å¹³å‡å›°æƒ‘åº¦: {scores['avg_perplexity']:.4f}")
        
        if overall_scores:
            avg_score = np.mean(overall_scores)
            print(f"\n  æ€»ä½“å¹³å‡ç²¾ç¡®åŒ¹é…: {avg_score:.4f}")
        
        # ä¿å­˜ç»“æœ
        import json
        with open("longbench_evaluation_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"  è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: longbench_evaluation_results.json")

class SmearTrainer:
    """SMEARè®­ç»ƒå™¨ - ä¿®å¤æ—©åœæœºåˆ¶ + FP16æ”¯æŒ"""
    
    def __init__(self, model, train_loader, val_loader, test_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.config = config
        
        # FP16é…ç½®
        self.use_fp16 = getattr(config, 'use_fp16', True)  # é»˜è®¤ä¸ºTrue
        self.scaler = GradScaler() if self.use_fp16 and torch.cuda.is_available() else None
        
        # ä¼˜åŒ–å™¨ - åªè®­ç»ƒAdapterå‚æ•°
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        self.optimizer = AdamW(trainable_params, lr=config.learning_rate, weight_decay=config.weight_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=config.warmup_steps,
            num_training_steps=config.total_steps
        )
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_stats = {
            'losses': [],
            'perplexities': [],
            'learning_rates': [],
            'test_loss': None,
            'test_perplexity': None,
            'best_val_loss': float('inf'),
            'best_epoch': -1,
            'early_stop_epoch': None,
            'fp16_enabled': self.use_fp16 and self.scaler is not None  # è®°å½•FP16çŠ¶æ€
        }
        
        # æ—©åœç›¸å…³å˜é‡
        self.patience = getattr(config, 'patience', 3)
        self.patience_counter = 0
        self.min_delta = getattr(config, 'min_delta', 1e-4)
        
        print(f"  åˆå§‹åŒ–SMEARè®­ç»ƒå™¨")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - FP16: {'å¯ç”¨' if self.train_stats['fp16_enabled'] else 'ç¦ç”¨'}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,}")
        print(f"   - æ—©åœè€å¿ƒå€¼: {self.patience} epochs")
        if self.train_stats['fp16_enabled']:
            print(f"   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (FP16)")

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch - æ”¯æŒFP16"""
        self.model.train()
        total_loss = 0
        
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        
        for batch_idx, batch in enumerate(progress_bar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # FP16å‰å‘ä¼ æ’­
            if self.use_fp16 and self.scaler is not None:
                with autocast():
                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs['loss']
                
                # FP16åå‘ä¼ æ’­å’Œæ¢¯åº¦ç¼©æ”¾
                self.optimizer.zero_grad()
                self.scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ªï¼ˆåœ¨ç¼©æ”¾åçš„æ¢¯åº¦ä¸Šï¼‰
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                
                # ä¼˜åŒ–å™¨æ­¥éª¤å’Œç¼©æ”¾å™¨æ›´æ–°
                self.scaler.step(self.optimizer)
                self.scaler.update()
                
            else:
                # æ™®é€šFP32è®­ç»ƒ
                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs['loss']
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                self.optimizer.step()
            
            # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä¸¤ç§æ¨¡å¼éƒ½éœ€è¦ï¼‰
            self.scheduler.step()
            
            # è®°å½•æŸå¤±
            total_loss += loss.item()
            
            # è¿›åº¦æ¡æ›´æ–°
            current_lr = self.scheduler.get_last_lr()[0]
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'LR': f'{current_lr:.2e}',
                'Patience': f'{self.patience_counter}/{self.patience}',
                'FP16': 'ON' if self.use_fp16 else 'OFF'
            })
            
            # å®šæœŸè®°å½•å­¦ä¹ ç‡
            if batch_idx % 100 == 0:
                self.train_stats['learning_rates'].append(current_lr)
        
        # è®°å½•epochç»Ÿè®¡
        avg_loss = total_loss / len(self.train_loader)
        self.train_stats['losses'].append(avg_loss)
        
        return avg_loss

    def validate(self, epoch):
        """éªŒè¯ - æ”¯æŒFP16"""
        self.model.eval()
        total_loss = 0
        total_perplexity = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # éªŒè¯æ—¶ä¹Ÿä½¿ç”¨FP16ä»¥å‡å°‘å†…å­˜å ç”¨
                if self.use_fp16:
                    with autocast():
                        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                        loss = outputs['loss']
                else:
                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs['loss']
                
                total_loss += loss.item()
                
                # è®¡ç®—å›°æƒ‘åº¦
                perplexity = torch.exp(torch.tensor(loss.item()))
                total_perplexity += perplexity.item()
        
        avg_loss = total_loss / len(self.val_loader)
        avg_perplexity = total_perplexity / len(self.val_loader)
        
        self.train_stats['perplexities'].append(avg_perplexity)
        
        return avg_loss, avg_perplexity

    def test(self, model_path=None):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ - æ”¯æŒFP16"""
        if self.test_loader is None:
            print("   æœªæä¾›æµ‹è¯•é›†ï¼Œè·³è¿‡æµ‹è¯•è¯„ä¼°")
            return None, None
        
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹è·¯å¾„ï¼Œåˆ™é‡æ–°åŠ è½½å®Œæ•´æ¨¡å‹
        if model_path is not None:
            self.load_complete_model(model_path)
            print(f"  åŠ è½½å®Œæ•´æ¨¡å‹è¿›è¡Œæµ‹è¯•: {model_path}")
        
        self.model.eval()
        total_loss = 0
        total_perplexity = 0
        
        print("  å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
        
        with torch.no_grad():
            for batch in tqdm(self.test_loader, desc='Testing'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # æµ‹è¯•æ—¶ä½¿ç”¨FP16
                if self.use_fp16:
                    with autocast():
                        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                        loss = outputs['loss']
                else:
                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs['loss']
                
                total_loss += loss.item()
                
                # è®¡ç®—å›°æƒ‘åº¦
                perplexity = torch.exp(torch.tensor(loss.item()))
                total_perplexity += perplexity.item()
        
        avg_loss = total_loss / len(self.test_loader)
        avg_perplexity = total_perplexity / len(self.test_loader)
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self.train_stats['test_loss'] = avg_loss
        self.train_stats['test_perplexity'] = avg_perplexity
        
        print(f"  æµ‹è¯•é›†ç»“æœ:")
        print(f"  - æµ‹è¯•æŸå¤±: {avg_loss:.4f}")
        print(f"  - æµ‹è¯•å›°æƒ‘åº¦: {avg_perplexity:.4f}")
        
        return avg_loss, avg_perplexity

    def save_complete_model(self, path):
        """ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹ + SMEARé€‚é…å™¨ï¼‰"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        save_path = os.path.join(self.config.output_dir, path)
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´æ¨¡å‹çŠ¶æ€
        complete_state_dict = {
            'model_state_dict': self.model.state_dict(),
            'config': self.config.__dict__,
            'training_stats': self.train_stats,
            'smear_adapters_only': False,
            'fp16_enabled': self.use_fp16  # ä¿å­˜FP16çŠ¶æ€
        }
        
        torch.save(complete_state_dict, os.path.join(save_path, 'complete_model.pth'))
        print(f"  ä¿å­˜å®Œæ•´æ¨¡å‹åˆ° {save_path}")
    
    def save_smear_adapters_only(self, path):
        """ä»…ä¿å­˜SMEARé€‚é…å™¨å‚æ•°ï¼ˆç”¨äºç»§ç»­è®­ç»ƒï¼‰"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        save_path = os.path.join(self.config.output_dir, path)
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜SMEARé€‚é…å™¨å‚æ•°
        model_state_dict = self.model.state_dict()
        smear_state_dict = {
            name: param for name, param in model_state_dict.items()
            if any(key in name for key in ['smear_adapters', 'adapter_alpha'])
        }
        
        adapter_only_state_dict = {
            'smear_adapters': smear_state_dict,
            'config': self.config.__dict__,
            'training_stats': self.train_stats,
            'smear_adapters_only': True  # æ ‡è®°è¿™æ˜¯ä»…é€‚é…å™¨
        }
        
        torch.save(adapter_only_state_dict, os.path.join(save_path, 'smear_adapters.pth'))
        print(f"ğŸ’¾ ä¿å­˜ {len(smear_state_dict)} ä¸ªSMEARé€‚é…å™¨å‚æ•°åˆ° {save_path}")
    
    def load_complete_model(self, model_path):
        """åŠ è½½å®Œæ•´æ¨¡å‹"""
        checkpoint_path = os.path.join(model_path, 'complete_model.pth')
        
        if not os.path.exists(checkpoint_path):
            print(f"  å®Œæ•´æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            print("   å°è¯•åŠ è½½ä»…é€‚é…å™¨ç‰ˆæœ¬...")
            return self.load_smear_adapters_only(model_path)
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # åŠ è½½å®Œæ•´æ¨¡å‹çŠ¶æ€
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # æ›´æ–°é…ç½®å’Œè®­ç»ƒç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
        if 'training_stats' in checkpoint:
            self.train_stats.update(checkpoint['training_stats'])
        
        print(f"  ä» {model_path} åŠ è½½å®Œæ•´æ¨¡å‹")
        return True
    
    def load_smear_adapters_only(self, model_path):
        """ä»…åŠ è½½SMEARé€‚é…å™¨å‚æ•°"""
        checkpoint_path = os.path.join(model_path, 'smear_adapters.pth')
        
        if not os.path.exists(checkpoint_path):
            print(f"  é€‚é…å™¨æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return False
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # è·å–å½“å‰æ¨¡å‹çŠ¶æ€å­—å…¸
        model_state_dict = self.model.state_dict()
        
        # åªæ›´æ–°SMEARç›¸å…³çš„å‚æ•°
        smear_adapters = checkpoint['smear_adapters']
        for name, param in smear_adapters.items():
            if name in model_state_dict:
                model_state_dict[name].copy_(param)
            else:
                print(f"   è·³è¿‡ä¸åŒ¹é…çš„å‚æ•°: {name}")
        
        # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        self.model.load_state_dict(model_state_dict)
        
        # æ›´æ–°è®­ç»ƒç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
        if 'training_stats' in checkpoint:
            self.train_stats.update(checkpoint['training_stats'])
        
        print(f"  ä» {model_path} åŠ è½½SMEARé€‚é…å™¨å‚æ•°")
        return True
    
    def check_early_stop(self, current_val_loss, best_val_loss, epoch):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ - ä¿®å¤ç‰ˆæœ¬"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—æ”¹å–„ï¼ˆè¶…è¿‡æœ€å°é˜ˆå€¼ï¼‰
        improvement = best_val_loss - current_val_loss
        
        if improvement > self.min_delta:
            # æœ‰æ˜¾è‘—æ”¹å–„ï¼Œé‡ç½®è®¡æ•°å™¨
            self.patience_counter = 0
            print(f"  éªŒè¯æŸå¤±æ”¹å–„: {improvement:.6f} > {self.min_delta}")
            return False
        else:
            # æ²¡æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¢åŠ è®¡æ•°å™¨
            self.patience_counter += 1
            print(f"  éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œè€å¿ƒè®¡æ•°: {self.patience_counter}/{self.patience}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è€å¿ƒé™åˆ¶
            if self.patience_counter >= self.patience:
                print(f"  æ—©åœè§¦å‘ï¼è¿ç»­ {self.patience} ä¸ªepochéªŒè¯æŸå¤±æœªæ”¹å–„")
                self.train_stats['early_stop_epoch'] = epoch
                return True
            
            return False
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹ - æ—©åœæœºåˆ¶"""
        best_val_loss = float('inf')
        best_epoch = -1
        
        print(" å¼€å§‹è®­ç»ƒ")
        
        for epoch in range(self.config.num_epochs):
            print(f"\nğŸ“ Epoch {epoch + 1}/{self.config.num_epochs}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_perplexity = self.validate(epoch)
            
            # ç®€åŒ–çš„è®­ç»ƒç»Ÿè®¡è¾“å‡º
            print(f"  è®­ç»ƒç»Ÿè®¡:")
            print(f"  - è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  - éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  - éªŒè¯å›°æƒ‘åº¦: {val_perplexity:.4f}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
            has_improvement = val_loss < best_val_loss - self.min_delta
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåªåœ¨æ€§èƒ½æå‡æ—¶ä¿å­˜ï¼‰
            if has_improvement:
                best_val_loss = val_loss
                best_epoch = epoch
                
                # ä¿å­˜å®Œæ•´æ¨¡å‹ç”¨äºæµ‹è¯•
                self.save_complete_model("best_smear_model")
                # åŒæ—¶ä¿å­˜é€‚é…å™¨å‚æ•°ç”¨äºç»§ç»­è®­ç»ƒ
                self.save_smear_adapters_only("best_smear_adapters")
                
                self.train_stats['best_val_loss'] = best_val_loss
                self.train_stats['best_epoch'] = best_epoch
                print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f}, Epoch: {epoch})")
            else:
                print(f"  éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œè·³è¿‡ä¿å­˜ (å½“å‰æœ€ä½³: {best_val_loss:.4f})")
            
            # æ£€æŸ¥æ—©åœæ¡ä»¶ - åªåœ¨æ²¡æœ‰æ”¹å–„æ—¶æ£€æŸ¥
            if not has_improvement and self.check_early_stop(val_loss, best_val_loss, epoch):
                print(f"   è®­ç»ƒåœ¨ Epoch {epoch} æå‰åœæ­¢")
                break
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            self.save_training_stats()
        
        # è®­ç»ƒç»“æŸååœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹
        print(f"\n{'='*50}")
        print("  è®­ç»ƒå®Œæˆï¼Œå¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")
        print(f"{'='*50}")
        
        test_loss, test_perplexity = self.test("best_smear_model")
        
        # æœ€ç»ˆæŠ¥å‘Š
        print(f"\n{'='*50}")
        print("  æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š:")
        print(f"{'='*50}")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f} (Epoch {best_epoch})")
        print(f"  æœ€ç»ˆè®­ç»ƒè½®æ•°: {len(self.train_stats['losses'])}")
        if self.train_stats['early_stop_epoch'] is not None:
            print(f"   æ—©åœè§¦å‘äº: Epoch {self.train_stats['early_stop_epoch']}")
        if test_loss is not None:
            print(f"  æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
            print(f"  æµ‹è¯•é›†å›°æƒ‘åº¦: {test_perplexity:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        self.save_final_report(best_val_loss, best_epoch, test_loss, test_perplexity)
        
        return best_val_loss
    
    def save_training_stats(self):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        with open(os.path.join(self.config.output_dir, 'smear_training_stats.json'), 'w') as f:
            json.dump(self.train_stats, f, indent=2)
    
    def save_final_report(self, best_val_loss, best_epoch, test_loss, test_perplexity):
        """ä¿å­˜æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š"""
        report = {
            'training_summary': {
                'best_validation_loss': best_val_loss,
                'best_epoch': best_epoch,
                'test_loss': test_loss,
                'test_perplexity': test_perplexity,
                'total_training_epochs': len(self.train_stats['losses']),
                'early_stop_epoch': self.train_stats['early_stop_epoch'],
                'final_learning_rate': self.train_stats['learning_rates'][-1] if self.train_stats['learning_rates'] else 0,
                'patience_used': self.patience_counter
            },
            'model_info': {
                'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),
                'device': str(self.device)
            },
            'early_stop_config': {
                'patience': self.patience,
                'min_delta': self.min_delta
            },
            'config': self.config.__dict__
        }
        
        report_path = os.path.join(self.config.output_dir, 'final_training_report.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"  æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

class IntegratedSmearTrainer(SmearTrainer):
    """é›†æˆLongBenchè¯„ä¼°çš„SMEARè®­ç»ƒå™¨"""
    
    def __init__(self, model, train_loader, val_loader, test_loader, config):
        super().__init__(model, train_loader, val_loader, test_loader, config)
        self.longbench_evaluator = None
    
    def setup_longbench_evaluator(self):
        """è®¾ç½®LongBenchè¯„ä¼°å™¨"""
        self.longbench_evaluator = LongBenchEvaluator(
            self.model, 
            self.tokenizer,  # éœ€è¦ç¡®ä¿tokenizerå¯ç”¨
            self.device
        )
        print("  LongBenchè¯„ä¼°å™¨å·²è®¾ç½®")
    
    def evaluate_on_longbench(self, max_samples_per_task=10):
        """åœ¨LongBenchä¸Šè¯„ä¼°æ¨¡å‹"""
        if self.longbench_evaluator is None:
            self.setup_longbench_evaluator()
        
        print("\n" + "="*50)
        print("  å¼€å§‹åœ¨LongBenchä¸Šè¯„ä¼°æ¨¡å‹...")
        print("="*50)
        
        results = self.longbench_evaluator.evaluate_all_tasks(max_samples_per_task)
        
        # å°†LongBenchç»“æœé›†æˆåˆ°è®­ç»ƒç»Ÿè®¡ä¸­
        if 'longbench_results' not in self.train_stats:
            self.train_stats['longbench_results'] = {}
        
        self.train_stats['longbench_results'][f'epoch_{len(self.train_stats["losses"])}'] = results
        
        return results
    
    def train_with_longbench_eval(self, longbench_eval_interval=2):
        """å¸¦LongBenchè¯„ä¼°çš„è®­ç»ƒæµç¨‹"""
        best_val_loss = float('inf')
        best_epoch = -1
        
        print("  å¼€å§‹å¸¦LongBenchè¯„ä¼°çš„SMEARè®­ç»ƒ")
        
        for epoch in range(self.config.num_epochs):
            print(f"\n  Epoch {epoch + 1}/{self.config.num_epochs}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_perplexity = self.validate(epoch)
            
            print(f"  è®­ç»ƒç»Ÿè®¡:")
            print(f"  - è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  - éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  - éªŒè¯å›°æƒ‘åº¦: {val_perplexity:.4f}")
            
            # å®šæœŸåœ¨LongBenchä¸Šè¯„ä¼°
            if (epoch + 1) % longbench_eval_interval == 0:
                self.evaluate_on_longbench(max_samples_per_task=10)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
            has_improvement = val_loss < best_val_loss - self.min_delta
            
            if has_improvement:
                best_val_loss = val_loss
                best_epoch = epoch
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_complete_model("best_smear_model")
                self.save_smear_adapters_only("best_smear_adapters")
                
                self.train_stats['best_val_loss'] = best_val_loss
                self.train_stats['best_epoch'] = best_epoch
                print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f}, Epoch: {epoch})")
            else:
                print(f"  éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œè·³è¿‡ä¿å­˜ (å½“å‰æœ€ä½³: {best_val_loss:.4f})")
            
            # æ£€æŸ¥æ—©åœ
            if not has_improvement and self.check_early_stop(val_loss, best_val_loss, epoch):
                print(f"   è®­ç»ƒåœ¨ Epoch {epoch} æå‰åœæ­¢")
                break
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            self.save_training_stats()
        
        # æœ€ç»ˆè¯„ä¼°
        print(f"\n{'='*50}")
        print("  è®­ç»ƒå®Œæˆï¼Œå¼€å§‹æœ€ç»ˆè¯„ä¼°...")
        print(f"{'='*50}")
        
        # åœ¨BookCorpusæµ‹è¯•é›†ä¸Šè¯„ä¼°
        test_loss, test_perplexity = self.test("best_smear_model")
        
        # åœ¨LongBenchä¸Šæœ€ç»ˆè¯„ä¼°
        final_longbench_results = self.evaluate_on_longbench(max_samples_per_task=20)
        
        # æœ€ç»ˆæŠ¥å‘Š
        print(f"\n{'='*50}")
        print("  æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š:")
        print(f"{'='*50}")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f} (Epoch {best_epoch})")
        print(f"  æœ€ç»ˆè®­ç»ƒè½®æ•°: {len(self.train_stats['losses'])}")
        if test_loss is not None:
            print(f"  BookCorpusæµ‹è¯•é›†:")
            print(f"  - æµ‹è¯•æŸå¤±: {test_loss:.4f}")
            print(f"  - æµ‹è¯•å›°æƒ‘åº¦: {test_perplexity:.4f}")
        
        if final_longbench_results:
            print(f"  LongBenchè¯„ä¼°:")
            for task, scores in final_longbench_results.items():
                print(f"  - {task}: ç²¾ç¡®åŒ¹é… = {scores.get('exact_match', 0):.4f}")
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        self.save_final_report_with_longbench(
            best_val_loss, best_epoch, test_loss, test_perplexity, final_longbench_results
        )
        
        return best_val_loss
    
    def save_final_report_with_longbench(self, best_val_loss, best_epoch, test_loss, test_perplexity, longbench_results):
        """ä¿å­˜åŒ…å«LongBenchç»“æœçš„æœ€ç»ˆæŠ¥å‘Š"""
        report = {
            'training_summary': {
                'best_validation_loss': best_val_loss,
                'best_epoch': best_epoch,
                'test_loss': test_loss,
                'test_perplexity': test_perplexity,
                'total_training_epochs': len(self.train_stats['losses']),
                'early_stop_epoch': self.train_stats['early_stop_epoch'],
            },
            'longbench_evaluation': longbench_results,
            'model_info': {
                'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),
                'device': str(self.device)
            },
            'dataset_info': {
                'training_data': 'BookCorpus',
                'evaluation_data': 'LongBench + BookCorpus Test Set'
            }
        }
        
        report_path = os.path.join(self.config.output_dir, 'final_training_report_with_longbench.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"  æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")