import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import GPT2Tokenizer

def get_bookcorpus_dataloaders(config):
    """ä¸“é—¨ä¸ºBookCorpusè®¾è®¡çš„æ•°æ®åŠ è½½å™¨"""
    tokenizer = GPT2Tokenizer.from_pretrained("/home/yang/gpt2-moe-adapter/gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    
    print("ğŸ“š åŠ è½½BookCorpusæ•°æ®é›†...")
    
    # åŠ è½½BookCorpus
    # try:
    dataset = load_dataset("bookcorpus", split="train", trust_remote_code=True)
    print(f"âœ… æˆåŠŸåŠ è½½BookCorpusï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
    # except Exception as e:
    #     print(f"âŒ åŠ è½½BookCorpuså¤±è´¥: {e}")
    #     # å›é€€åˆ°è¾ƒå°çš„ç‰ˆæœ¬
    #     try:
    #         dataset = load_dataset("md_gender", "bookcorpus", split="train")
    #         print(f"âœ… ä½¿ç”¨å¤‡ç”¨BookCorpusç‰ˆæœ¬ï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
    #     except:
    #         raise ValueError("æ— æ³•åŠ è½½BookCorpusæ•°æ®é›†")
    
    # è‡ªå®šä¹‰åˆ†å‰²ï¼šè®­ç»ƒé›†80%ï¼ŒéªŒè¯é›†10%ï¼Œæµ‹è¯•é›†10%
    total_size = len(dataset)
    train_size = int(0.8 * total_size)
    val_size = int(0.1 * total_size)
    test_size = total_size - train_size - val_size
    
    # åˆ†å‰²æ•°æ®é›†
    train_dataset = dataset.select(range(train_size))
    val_dataset = dataset.select(range(train_size, train_size + val_size))
    test_dataset = dataset.select(range(train_size + val_size, total_size))
    
    print(f"ğŸ“Š æ•°æ®é›†åˆ†å‰²:")
    print(f"   - è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬") 
    print(f"   - æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    def tokenize_function(examples):
        """åˆ†è¯å‡½æ•° - é’ˆå¯¹BookCorpusä¼˜åŒ–"""
        # è¿æ¥æ–‡æœ¬å¹¶åˆ†è¯
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=config.max_length,
            return_tensors=None
        )
        return tokenized
    
    # åˆ†è¯å¤„ç†
    print("ğŸ”¤ å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯å¤„ç†...")
    tokenized_train = train_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=train_dataset.column_names,
        desc="Tokenizing training set"
    )
    
    tokenized_val = val_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=val_dataset.column_names,
        desc="Tokenizing validation set"
    )
    
    tokenized_test = test_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=test_dataset.column_names,
        desc="Tokenizing test set"
    )
    
    # ä¸ºè¯­è¨€å»ºæ¨¡å‡†å¤‡labels
    def group_texts(examples):
        """å°†æ–‡æœ¬åˆ†ç»„ä¸ºå›ºå®šé•¿åº¦çš„å—"""
        concatenated = {k: sum(examples[k], []) for k in examples.keys()}
        total_length = len(concatenated[list(examples.keys())[0]])
        
        # ä¸¢å¼ƒå‰©ä½™éƒ¨åˆ†
        if total_length >= config.max_length:
            total_length = (total_length // config.max_length) * config.max_length
        
        result = {
            k: [t[i : i + config.max_length] for i in range(0, total_length, config.max_length)]
            for k, t in concatenated.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    
    # åº”ç”¨æ–‡æœ¬åˆ†ç»„
    print("ğŸ“¦ åˆ†ç»„æ–‡æœ¬ä¸ºå›ºå®šé•¿åº¦å—...")
    tokenized_train = tokenized_train.map(
        group_texts,
        batched=True,
        desc="Grouping training texts"
    )
    
    tokenized_val = tokenized_val.map(
        group_texts,
        batched=True,
        desc="Grouping validation texts"
    )
    
    tokenized_test = tokenized_test.map(
        group_texts,
        batched=True,
        desc="Grouping test texts"
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    def collate_fn(batch):
        return {
            'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),
            'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),
            'labels': torch.stack([torch.tensor(item['labels']) for item in batch])
        }
    
    train_loader = DataLoader(
        tokenized_train,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn
    )
    
    val_loader = DataLoader(
        tokenized_val,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    test_loader = DataLoader(
        tokenized_test,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    print("âœ… BookCorpusæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    return train_loader, val_loader, test_loader, tokenizer