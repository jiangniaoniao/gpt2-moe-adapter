import torch
from torch.utils.data import DataLoader
from datasets import load_dataset, concatenate_datasets
from transformers import GPT2Tokenizer
import random

def get_dataloaders(config):
    """æ”¯æŒå¤šæ ·åŒ–æ•°æ®é›†æ··åˆçš„æ•°æ®åŠ è½½å™¨"""
    tokenizer = GPT2Tokenizer.from_pretrained("/home/yang/gpt2-moe-adapter/gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    
    dataset_mode = getattr(config, 'dataset_mode', 'single')
    
    if dataset_mode == 'mixed':
        return get_mixed_dataloaders(config, tokenizer)
    else:
        return get_wikitext_dataloaders(config, tokenizer)

def get_mixed_dataloaders(config, tokenizer):
    """ä¿®å¤æ ·æœ¬æ•°é‡é—®é¢˜çš„æ··åˆæ•°æ®é›†åŠ è½½å™¨"""
    print("ğŸš€ åŠ è½½æ··åˆå¤šæ ·åŒ–æ•°æ®é›†...")
    
    # æ•°æ®é›†é…ç½®
    dataset_mix = getattr(config, 'dataset_mix', [
        # åŸºç¡€æŒ‡ä»¤æ•°æ® (40%)
        ("tatsu-lab/alpaca", None, 0.25, "instruction"),
        ("databricks/databricks-dolly-15k", None, 0.15, "instruction"),
        
        # å­¦ç§‘çŸ¥è¯†æ•°æ® (40%)
        ("qwedsacf/ivi-mmlu", None, 0.2, "knowledge"),
        ("allenai/sciq", None, 0.2, "knowledge"),
        
        # æ¨ç†æ•°æ® (20%)
        ("gsm8k", "main", 0.1, "reasoning"),
        ("tau/commonsense_qa", None, 0.1, "reasoning"),
    ])
    
    # ç›®æ ‡æ€»æ ·æœ¬æ•°
    target_total_samples = getattr(config, 'target_total_samples', 50000)
    
    all_datasets = []
    dataset_info = []
    
    for i, (dataset_name, dataset_config, weight, dataset_type) in enumerate(dataset_mix):
        try:
            print(f"  ğŸ“‚ åŠ è½½æ•°æ®é›† {i+1}/{len(dataset_mix)}: {dataset_name}")
            
            # åŠ è½½æ•°æ®é›†
            if dataset_config:
                dataset = load_dataset(dataset_name, dataset_config)
            else:
                dataset = load_dataset(dataset_name)
            
            # è·å–è®­ç»ƒåˆ†å‰²
            if "train" in dataset:
                train_data = dataset["train"]
            elif "training" in dataset:
                train_data = dataset["training"]
            else:
                first_split = list(dataset.keys())[0]
                train_data = dataset[first_split]
            
            print(f"    åŸå§‹æ•°æ®é›†å¤§å°: {len(train_data)}")
            
            # æ ¼å¼åŒ–æ•°æ®é›† - ä½¿ç”¨æ–°çš„å¤„ç†å‡½æ•°
            formatted_dataset = format_and_process_dataset(
                train_data, dataset_type, tokenizer, config
            )
            
            if len(formatted_dataset) == 0:
                print(f"    âš ï¸  æ ¼å¼åŒ–åæ— æœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡")
                continue
            
            # è®¡ç®—ç›®æ ‡æ ·æœ¬æ•° - æ›´åˆç†çš„é‡‡æ ·ç­–ç•¥
            target_samples = min(
                int(target_total_samples * weight),  # ç›´æ¥æŒ‰æƒé‡è®¡ç®—
                len(formatted_dataset)
            )
            
            # é‡‡æ ·
            if target_samples < len(formatted_dataset):
                indices = random.sample(range(len(formatted_dataset)), target_samples)
                formatted_dataset = formatted_dataset.select(indices)
            
            all_datasets.append(formatted_dataset)
            dataset_info.append({
                'name': dataset_name,
                'type': dataset_type,
                'weight': weight,
                'samples': len(formatted_dataset),
                'original_samples': len(train_data)
            })
            
            print(f"    âœ… æˆåŠŸåŠ è½½ {len(formatted_dataset)}/{len(train_data)} ä¸ªæ ·æœ¬")
            
        except Exception as e:
            print(f"    âŒ åŠ è½½å¤±è´¥: {e}")
            continue
    
    if not all_datasets:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®é›†ï¼")
    
    print("ğŸ”— åˆå¹¶æ•°æ®é›†...")
    combined_dataset = concatenate_datasets(all_datasets)
    
    # å¦‚æœæ€»æ ·æœ¬æ•°ä»å¤ªå°‘ï¼Œè€ƒè™‘é‡å¤é‡‡æ ·
    if len(combined_dataset) < 20000:
        print(f"âš ï¸  æ€»æ ·æœ¬æ•°è¾ƒå°‘ ({len(combined_dataset)})ï¼Œè€ƒè™‘é‡å¤é‡‡æ ·...")
        repeat_times = max(1, 30000 // len(combined_dataset))
        combined_dataset = concatenate_datasets([combined_dataset] * repeat_times)
        print(f"    é‡å¤é‡‡æ ·å: {len(combined_dataset)} æ ·æœ¬")
    
    # æ‰“ä¹±æ•°æ®
    combined_dataset = combined_dataset.shuffle(seed=42)
    
    # æ‰“å°æ•°æ®é›†ç»Ÿè®¡
    print("\nğŸ“Š æ•°æ®é›†æ··åˆç»Ÿè®¡:")
    for info in dataset_info:
        actual_weight = info['samples'] / len(combined_dataset) if len(combined_dataset) > 0 else 0
        print(f"   - {info['name']} ({info['type']}): {info['samples']} æ ·æœ¬ "
              f"(ç›®æ ‡æƒé‡: {info['weight']:.2f}, å®é™…æƒé‡: {actual_weight:.2f})")
    print(f"   ğŸ“ˆ æ€»æ ·æœ¬æ•°: {len(combined_dataset)}")
    
    # åˆ†å‰²æ•°æ®é›†
    total_size = len(combined_dataset)
    train_size = int(0.8 * total_size)
    val_size = int(0.1 * total_size)
    test_size = total_size - train_size - val_size
    
    train_dataset = combined_dataset.select(range(train_size))
    val_dataset = combined_dataset.select(range(train_size, train_size + val_size))
    test_dataset = combined_dataset.select(range(train_size + val_size, total_size))
    
    print(f"\nğŸ“‹ æœ€ç»ˆæ•°æ®é›†åˆ†å‰²:")
    print(f"   - è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬") 
    print(f"   - æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    def collate_fn(batch):
        return {
            'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),
            'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),
            'labels': torch.stack([torch.tensor(item['labels']) for item in batch])
        }
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    print("ğŸ‰ æ··åˆæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼")
    return train_loader, val_loader, test_loader, tokenizer

def format_and_process_dataset(dataset, dataset_type, tokenizer, config):
    """æ–°çš„æ ¼å¼åŒ–å¤„ç†å‡½æ•° - ä¿æŒæ ·æœ¬ç‹¬ç«‹æ€§"""
    
    # å…ˆæ ¼å¼åŒ–æ–‡æœ¬
    formatted_dataset = format_dataset_text(dataset, dataset_type)
    
    if len(formatted_dataset) == 0:
        return formatted_dataset
    
    # ç„¶åè¿›è¡Œåˆ†è¯å¤„ç†
    return process_dataset_with_padding(formatted_dataset, tokenizer, config)

def format_dataset_text(dataset, dataset_type):
    """ä»…æ ¼å¼åŒ–æ–‡æœ¬å†…å®¹ï¼Œä¿æŒæ ·æœ¬ç‹¬ç«‹æ€§"""
    
    def instruction_format(example):
        # Alpacaæ ¼å¼
        if 'instruction' in example and 'output' in example:
            input_text = f"Instruction: {example['instruction']}\n"
            if example.get('input', '').strip():
                input_text += f"Input: {example['input']}\n"
            input_text += f"Response: {example['output']}"
            return {"text": input_text}
        
        # Dollyæ ¼å¼
        elif 'instruction' in example and 'response' in example:
            input_text = f"Instruction: {example['instruction']}\n"
            if example.get('context', '').strip():
                input_text += f"Context: {example['context']}\n"
            input_text += f"Response: {example['response']}"
            return {"text": input_text}
        
        # å…¶ä»–æ ¼å¼
        elif 'question' in example and 'answer' in example:
            return {"text": f"Q: {example['question']}\nA: {example['answer']}"}
        elif 'text' in example:
            return {"text": example['text']}
        
        return {"text": str(example)}
    
    def knowledge_format(example):
        # MMLUæ›¿ä»£æ ¼å¼
        if 'input' in example and 'target' in example:
            return {"text": f"Question: {example['input']}\nAnswer: {example['target']}"}
        
        # SciQæ ¼å¼
        elif 'question' in example and 'correct_answer' in example:
            question = example['question']
            choices = [example['distractor1'], example['distractor2'], 
                      example['distractor3'], example['correct_answer']]
            random.shuffle(choices)
            answer = example['correct_answer']
            
            choices_text = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(choices)])
            text = f"Question: {question}\nOptions:\n{choices_text}\nAnswer: {answer}"
            return {"text": text}
        
        elif 'question' in example and 'answer' in example:
            return {"text": f"Q: {example['question']}\nA: {example['answer']}"}
        
        return {"text": str(example)}
    
    def reasoning_format(example):
        # GSM8Kæ ¼å¼
        if 'question' in example and 'answer' in example:
            return {"text": f"Math Problem: {example['question']}\nSolution: {example['answer']}"}
        
        # CommonsenseQAæ ¼å¼
        elif 'question' in example and 'choices' in example and 'answerKey' in example:
            question = example['question']
            choices = example['choices']
            answer_key = example['answerKey']
            
            if isinstance(choices, dict) and 'text' in choices:
                choices = choices['text']
            
            if isinstance(choices, list):
                choices_text = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(choices)])
                answer_idx = ord(answer_key) - ord('A') if len(answer_key) == 1 else int(answer_key)
                answer = choices[answer_idx] if answer_idx < len(choices) else answer_key
                text = f"Question: {question}\nOptions:\n{choices_text}\nAnswer: {answer}"
                return {"text": text}
        
        return {"text": str(example)}
    
    def lm_format(example):
        if 'text' in example:
            text = example['text'].strip()
            if text and (not text.startswith("=") or len(text) > 10):
                return {"text": text}
        return {"text": ""}
    
    # é€‰æ‹©æ ¼å¼åŒ–å‡½æ•°
    if dataset_type == "instruction":
        format_func = instruction_format
    elif dataset_type == "knowledge":
        format_func = knowledge_format
    elif dataset_type == "reasoning":
        format_func = reasoning_format
    else:
        format_func = lm_format
    
    try:
        formatted = dataset.map(format_func)
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        formatted = formatted.filter(lambda x: x['text'] and x['text'].strip())
        return formatted
    except Exception as e:
        print(f"    âš ï¸  æ•°æ®æ ¼å¼åŒ–å¤±è´¥: {e}, ä½¿ç”¨åŸå§‹æ–‡æœ¬")
        # å°è¯•ä½¿ç”¨åŸå§‹æ–‡æœ¬
        if 'text' in dataset.column_names:
            return dataset.filter(lambda x: x['text'] and x['text'].strip())
        else:
            # å¦‚æœè¿textåˆ—éƒ½æ²¡æœ‰ï¼Œåˆ›å»ºä¸€ä¸ª
            return dataset.map(lambda x: {"text": str(x)}).filter(lambda x: x['text'] and x['text'].strip())

def process_dataset_with_padding(dataset, tokenizer, config):
    """ä½¿ç”¨å¡«å……è€Œä¸æ˜¯åˆ†ç»„æ¥å¤„ç†æ•°æ®é›† - ä¿æŒæ ·æœ¬æ•°é‡"""
    
    def tokenize_function(examples):
        texts = [text for text in examples["text"] if text and text.strip()]
        
        if not texts:
            return {"input_ids": [], "attention_mask": []}
        
        # ä½¿ç”¨å¡«å……åˆ°æœ€å¤§é•¿åº¦
        tokenized = tokenizer(
            texts,
            truncation=True,
            padding='max_length',  # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨å¡«å……è€Œä¸æ˜¯åˆ†ç»„
            max_length=config.max_length,
            return_tensors=None
        )
        
        # ä¸ºè¯­è¨€å»ºæ¨¡è®¾ç½®labels
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        return tokenized
    
    # åˆ†è¯å¤„ç†
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names,
        desc="Tokenizing dataset with padding"
    )
    
    # è¿‡æ»¤æ‰å¤ªçŸ­çš„åºåˆ—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    tokenized_dataset = tokenized_dataset.filter(
        lambda x: len(x.get('input_ids', [])) > 10
    )
    
    return tokenized_dataset

def format_dataset(dataset, dataset_type, tokenizer, config):
    """å‘åå…¼å®¹çš„åŒ…è£…å‡½æ•°"""
    return format_and_process_dataset(dataset, dataset_type, tokenizer, config)

def format_instruction_data(dataset, tokenizer, config):
    return format_and_process_dataset(dataset, "instruction", tokenizer, config)

def format_knowledge_data(dataset, tokenizer, config):
    return format_and_process_dataset(dataset, "knowledge", tokenizer, config)

def format_reasoning_data(dataset, tokenizer, config):
    return format_and_process_dataset(dataset, "reasoning", tokenizer, config)

def format_lm_data(dataset, tokenizer, config):
    return format_and_process_dataset(dataset, "lm", tokenizer, config)

# ä¿ç•™åŸæœ‰çš„WikiTextæ•°æ®åŠ è½½å™¨ï¼ˆä¿æŒä¸å˜ï¼‰
def get_wikitext_dataloaders(config, tokenizer):
    """WikiTextæ•°æ®åŠ è½½å™¨"""
    print("  åŠ è½½WikiTextæ•°æ®é›†...")
    
    dataset_config = getattr(config, 'dataset_config', 'wikitext-2-raw-v1')
    dataset = load_dataset("wikitext", dataset_config)
    
    train_dataset = dataset["train"]
    val_dataset = dataset["validation"]
    test_dataset = dataset["test"] if "test" in dataset else dataset["validation"]
    
    print(f"  æˆåŠŸåŠ è½½WikiText:")
    print(f"   - è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"   - æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    def wikitext_tokenize_function(examples):
        texts = []
        for text in examples["text"]:
            if text.strip() and not text.strip().startswith("="):
                texts.append(text.strip())
        
        if not texts:
            return {"input_ids": [], "attention_mask": []}
        
        tokenized = tokenizer(
            texts,
            truncation=True,
            padding='max_length',  # ä¹Ÿæ”¹ä¸ºå¡«å……
            max_length=config.max_length,
            return_tensors=None
        )
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized
    
    tokenized_train = train_dataset.map(
        wikitext_tokenize_function,
        batched=True,
        remove_columns=train_dataset.column_names,
        desc="Tokenizing WikiText training set"
    )
    
    tokenized_val = val_dataset.map(
        wikitext_tokenize_function,
        batched=True,
        remove_columns=val_dataset.column_names,
        desc="Tokenizing WikiText validation set"
    )
    
    tokenized_test = test_dataset.map(
        wikitext_tokenize_function,
        batched=True,
        remove_columns=test_dataset.column_names,
        desc="Tokenizing WikiText test set"
    )
    
    # ç§»é™¤åˆ†ç»„æ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨å¡«å……åçš„æ•°æ®
    
    def wikitext_collate_fn(batch):
        valid_batch = [item for item in batch if len(item['input_ids']) > 0]
        
        if not valid_batch:
            return {
                'input_ids': torch.empty((0, config.max_length), dtype=torch.long),
                'attention_mask': torch.empty((0, config.max_length), dtype=torch.long),
                'labels': torch.empty((0, config.max_length), dtype=torch.long)
            }
        
        return {
            'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in valid_batch]),
            'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in valid_batch]),
            'labels': torch.stack([torch.tensor(item['labels']) for item in valid_batch])
        }
    
    train_loader = DataLoader(
        tokenized_train,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=wikitext_collate_fn
    )
    
    val_loader = DataLoader(
        tokenized_val,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=wikitext_collate_fn
    )
    
    test_loader = DataLoader(
        tokenized_test,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=wikitext_collate_fn
    )
    
    print("  WikiTextæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    return train_loader, val_loader, test_loader, tokenizer