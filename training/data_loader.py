import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import GPT2Tokenizer

def get_dataloaders(config):
    """åŠ è½½æ•°æ®é›† - åŒ…å«è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†"""
    tokenizer = GPT2Tokenizer.from_pretrained("/home/yang/gpt2-moe-adapter/gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(config.dataset_name, config.dataset_config)
    
    def tokenize_function(examples):
        # è¿æ¥æ–‡æœ¬å¹¶åˆ†è¯
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=config.max_length,
            return_tensors=None
        )
        return tokenized
    
    # åˆ†è¯å¤„ç†
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset["train"].column_names,
    )
    
    # ä¸ºè¯­è¨€å»ºæ¨¡å‡†å¤‡labels
    def group_texts(examples):
        concatenated = {k: sum(examples[k], []) for k in examples.keys()}
        total_length = len(concatenated[list(examples.keys())[0]])
        
        # æˆ‘ä»¬ä¸¢å¼ƒå‰©ä½™éƒ¨åˆ†ï¼Œä½†å¦‚æœæ•°æ®é›†è¶³å¤Ÿå¤§åˆ™æ²¡é—®é¢˜
        if total_length >= config.max_length:
            total_length = (total_length // config.max_length) * config.max_length
        
        result = {
            k: [t[i : i + config.max_length] for i in range(0, total_length, config.max_length)]
            for k, t in concatenated.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    
    tokenized_datasets = tokenized_datasets.map(
        group_texts,
        batched=True,
        desc="Grouping texts in chunks of 1024",
    )
    
    # åˆ›å»ºä¸‰ä¸ªæ•°æ®åŠ è½½å™¨ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    train_dataset = tokenized_datasets["train"]
    
    # ä¼˜å…ˆä½¿ç”¨validationä½œä¸ºéªŒè¯é›†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨éƒ¨åˆ†testé›†
    if "validation" in tokenized_datasets:
        val_dataset = tokenized_datasets["validation"]
        test_dataset = tokenized_datasets["test"] if "test" in tokenized_datasets else None
    else:
        # å¦‚æœæ²¡æœ‰validationï¼Œå°†testé›†åˆ†å‰²ä¸ºéªŒè¯é›†å’Œæµ‹è¯•é›†
        test_split = tokenized_datasets["test"]
        split_ratio = getattr(config, 'val_test_split_ratio', 0.5)
        split_idx = int(len(test_split) * split_ratio)
        
        val_dataset = test_split.select(range(split_idx))
        test_dataset = test_split.select(range(split_idx, len(test_split)))
    
    # è®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=lambda batch: {
            'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),
            'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),
            'labels': torch.stack([torch.tensor(item['labels']) for item in batch])
        }
    )
    
    # éªŒè¯æ•°æ®åŠ è½½å™¨
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=lambda batch: {
            'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),
            'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),
            'labels': torch.stack([torch.tensor(item['labels']) for item in batch])
        }
    )
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆå¦‚æœæµ‹è¯•é›†å­˜åœ¨ï¼‰
    test_loader = None
    if test_dataset is not None:
        test_loader = DataLoader(
            test_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            collate_fn=lambda batch: {
                'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),
                'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),
                'labels': torch.stack([torch.tensor(item['labels']) for item in batch])
            }
        )
    
    # æ‰“å°æ•°æ®é›†ä¿¡æ¯
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   - è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"   - éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
    if test_loader:
        print(f"   - æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
    else:
        print(f"   - æµ‹è¯•é›†: æœªæä¾›")
    
    return train_loader, val_loader, test_loader, tokenizer

def get_wikitext_dataloaders_with_custom_split(config, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):
    """å¯é€‰ï¼šè‡ªå®šä¹‰æ•°æ®é›†åˆ†å‰²æ¯”ä¾‹"""
    assert train_ratio + val_ratio + test_ratio == 1.0, "åˆ†å‰²æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1"
    
    tokenizer = GPT2Tokenizer.from_pretrained("/home/yang/gpt2-moe-adapter/gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(config.dataset_name, config.dataset_config)
    
    def tokenize_function(examples):
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=config.max_length,
            return_tensors=None
        )
        return tokenized
    
    # åˆ†è¯å¤„ç†
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset["train"].column_names,
    )
    
    # ä¸ºè¯­è¨€å»ºæ¨¡å‡†å¤‡labels
    def group_texts(examples):
        concatenated = {k: sum(examples[k], []) for k in examples.keys()}
        total_length = len(concatenated[list(examples.keys())[0]])
        
        if total_length >= config.max_length:
            total_length = (total_length // config.max_length) * config.max_length
        
        result = {
            k: [t[i : i + config.max_length] for i in range(0, total_length, config.max_length)]
            for k, t in concatenated.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    
    tokenized_datasets = tokenized_datasets.map(
        group_texts,
        batched=True,
        desc="Grouping texts in chunks of 1024",
    )
    
    # è‡ªå®šä¹‰åˆ†å‰²
    train_dataset = tokenized_datasets["train"]
    val_dataset = tokenized_datasets["validation"]
    test_dataset = tokenized_datasets["test"]

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=lambda batch: {
            'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),
            'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),
            'labels': torch.stack([torch.tensor(item['labels']) for item in batch])
        }
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=lambda batch: {
            'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),
            'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),
            'labels': torch.stack([torch.tensor(item['labels']) for item in batch])
        }
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=lambda batch: {
            'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),
            'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),
            'labels': torch.stack([torch.tensor(item['labels']) for item in batch])
        }
    )
    
    print(f"ğŸ“Š è‡ªå®šä¹‰æ•°æ®é›†åˆ†å‰²:")
    print(f"   - è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"   - éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
    print(f"   - æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
    
    return train_loader, val_loader, test_loader, tokenizer