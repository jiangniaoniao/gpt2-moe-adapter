import torch
import torch.nn as nn
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import os
import json

class MoETrainer:
    """MoE Adapterè®­ç»ƒå™¨"""
    
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        
        # ä¼˜åŒ–å™¨ - åªè®­ç»ƒAdapterå‚æ•°
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        self.optimizer = AdamW(trainable_params, lr=config.learning_rate, weight_decay=config.weight_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=config.warmup_steps,
            num_training_steps=config.total_steps
        )
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_stats = {
            'losses': [],
            'lm_losses': [],
            'load_balancing_losses': [],
            'perplexities': []
        }
        
        print(f"ğŸš€ åˆå§‹åŒ–è®­ç»ƒå™¨")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,}")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_lm_loss = 0
        total_balance_loss = 0
        
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        
        for batch_idx, batch in enumerate(progress_bar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs['loss']
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
            
            self.optimizer.step()
            self.scheduler.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            if outputs['lm_loss'] is not None:
                total_lm_loss += outputs['lm_loss'].item()
            if outputs['load_balancing_loss'] is not None:
                total_balance_loss += outputs['load_balancing_loss'].item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'LM Loss': f'{outputs["lm_loss"].item() if outputs["lm_loss"] is not None else 0:.4f}',
                'Balance Loss': f'{outputs["load_balancing_loss"].item() if outputs["load_balancing_loss"] is not None else 0:.4f}'
            })
            
            # è®°å½•è·¯ç”±ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯100ä¸ªbatchï¼‰
            if batch_idx % 100 == 0 and outputs['router_metrics']:
                self._log_router_metrics(outputs['router_metrics'], epoch, batch_idx)
        
        # è®°å½•epochç»Ÿè®¡
        avg_loss = total_loss / len(self.train_loader)
        avg_lm_loss = total_lm_loss / len(self.train_loader)
        avg_balance_loss = total_balance_loss / len(self.train_loader)
        
        self.train_stats['losses'].append(avg_loss)
        self.train_stats['lm_losses'].append(avg_lm_loss)
        self.train_stats['load_balancing_losses'].append(avg_balance_loss)
        
        return avg_loss, avg_lm_loss, avg_balance_loss
    
    def _log_router_metrics(self, router_metrics, epoch, batch_idx):
        """è®°å½•è·¯ç”±å™¨ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š Epoch {epoch}, Batch {batch_idx} - ä¸“å®¶ä½¿ç”¨æƒ…å†µ:")
        for metric in router_metrics:
            layer = metric['layer']
            expert_usage = metric['expert_usage']
            print(f"  å±‚ {layer}: {expert_usage.cpu().detach().numpy().round(4)}")
    
    def validate(self, epoch):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        total_perplexity = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                total_loss += outputs['loss'].item()
                
                # è®¡ç®—å›°æƒ‘åº¦
                if outputs['lm_loss'] is not None:
                    perplexity = torch.exp(torch.tensor(outputs['lm_loss']))
                    total_perplexity += perplexity.item()
        
        avg_loss = total_loss / len(self.val_loader)
        avg_perplexity = total_perplexity / len(self.val_loader)
        
        self.train_stats['perplexities'].append(avg_perplexity)
        
        return avg_loss, avg_perplexity
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_val_loss = float('inf')
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒMoE Adapteræ¨¡å‹")
        
        for epoch in range(self.config.num_epochs):
            print(f"\nğŸ“ Epoch {epoch + 1}/{self.config.num_epochs}")
            
            # è®­ç»ƒ
            train_loss, train_lm_loss, train_balance_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_perplexity = self.validate(epoch)
            
            print(f"ğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
            print(f"  - æ€»æŸå¤±: {train_loss:.4f}")
            print(f"  - LMæŸå¤±: {train_lm_loss:.4f}") 
            print(f"  - è´Ÿè½½å‡è¡¡æŸå¤±: {train_balance_loss:.4f}")
            print(f"  - éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  - éªŒè¯å›°æƒ‘åº¦: {val_perplexity:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self.save_model(f"best_model_epoch_{epoch}")
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f})")
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            self.save_training_stats()
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        save_path = os.path.join(self.config.output_dir, path)
        os.makedirs(save_path, exist_ok=True)
        
        # åªä¿å­˜Adapterå‚æ•°
        adapter_state_dict = {
            name: param for name, param in self.model.state_dict().items()
            if 'moe_adapters' in name
        }
        
        torch.save(adapter_state_dict, os.path.join(save_path, 'adapter_weights.pth'))
        
        # ä¿å­˜é…ç½®
        with open(os.path.join(save_path, 'config.json'), 'w') as f:
            json.dump(self.config.__dict__, f, indent=2)
    
    def save_training_stats(self):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        with open(os.path.join(self.config.output_dir, 'training_stats.json'), 'w') as f:
            json.dump(self.train_stats, f, indent=2)