import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import torch
import torch.nn as nn
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer
from peft import get_peft_model, LoraConfig, TaskType
import argparse
from dataclasses import dataclass, field
import os
import numpy as np
from training.data_loader import get_dataloaders
from tqdm.auto import tqdm  # æ·»åŠ tqdm

@dataclass
class LoRAConfig:
    """LoRAå¾®è°ƒé…ç½®"""
    base_model: str = "/home/yang/gpt2-moe-adapter/gpt2"
    dataset_mode: str = "mixed"  # 'mixed' æˆ– 'single'
    batch_size: int = 4
    max_length: int = 512
    learning_rate: float = 5e-4
    num_epochs: int = 3
    warmup_steps: int = 500
    logging_steps: int = 100
    eval_steps: int = 500
    save_steps: int = 1000
    
    # æ—©åœé…ç½®
    early_stopping_patience: int = 3  # å®¹å¿çš„è¯„ä¼°æ¬¡æ•°
    early_stopping_threshold: float = 0.001  # æ”¹å–„é˜ˆå€¼
    
    # LoRAé…ç½®
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    lora_target_modules: tuple = ("c_attn", "c_proj")  # GPT-2çš„æ³¨æ„åŠ›æŠ•å½±å±‚
    
    # æ•°æ®é›†æ··åˆé…ç½®
    dataset_mix: list = field(
        default_factory=lambda: [
            # åŸºç¡€æŒ‡ä»¤æ•°æ®
            ("tatsu-lab/alpaca", None, 0.3, "instruction"),
            ("databricks/databricks-dolly-15k", None, 0.1, "instruction"),
            
            # å­¦ç§‘çŸ¥è¯†æ•°æ®  
            ("cais/mmlu", "all", 0.15, "knowledge"),
            ("allenai/ai2_arc", "ARC-Challenge", 0.15, "knowledge"),
            ("derek-thomas/ScienceQA", None, 0.1, "knowledge"),
            
            # æ¨ç†æ•°æ®
            ("gsm8k", "main", 0.1, "reasoning"),
            ("tau/commonsense_qa", None, 0.1, "reasoning"),
            
            # WikiTextåŸºç¡€è¯­è¨€å»ºæ¨¡
            ("wikitext", "wikitext-2-raw-v1", 0.1, "lm")
        ]
    )
    target_total_samples: int = 50000

class EarlyStoppingCallback:
    """æ—©åœå›è°ƒå‡½æ•°"""
    
    def __init__(self, patience=3, min_delta=0.001, save_path="./best_model"):
        self.patience = patience
        self.min_delta = min_delta
        self.save_path = save_path
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        
        # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨
        os.makedirs(save_path, exist_ok=True)
        
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """åœ¨è¯„ä¼°åè°ƒç”¨"""
        if metrics is None or 'eval_loss' not in metrics:
            return
        
        current_loss = metrics['eval_loss']
        
        if self.best_loss is None:
            # ç¬¬ä¸€æ¬¡è¯„ä¼°
            self.best_loss = current_loss
            self.save_checkpoint(args, state, control)
        elif current_loss < self.best_loss - self.min_delta:
            # æœ‰æ˜¾è‘—æ”¹å–„
            self.best_loss = current_loss
            self.counter = 0
            self.save_checkpoint(args, state, control)
            print(f"ğŸ¯ æ¨¡å‹æ”¹å–„! éªŒè¯æŸå¤±: {current_loss:.4f} (æœ€ä½³: {self.best_loss:.4f})")
        else:
            # æ²¡æœ‰æ”¹å–„
            self.counter += 1
            print(f"â³ æ—©åœè®¡æ•°: {self.counter}/{self.patience}, å½“å‰æŸå¤±: {current_loss:.4f}, æœ€ä½³æŸå¤±: {self.best_loss:.4f}")
            
            if self.counter >= self.patience:
                print("ğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶!")
                self.early_stop = True
                control.should_training_stop = True
    
    def save_checkpoint(self, args, state, control):
        """ä¿å­˜æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹"""
        print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ (æŸå¤±: {self.best_loss:.4f})")
        
    def on_step_end(self, args, state, control, **kwargs):
        """åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ç»“æŸæ—¶æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢"""
        if self.early_stop:
            control.should_training_stop = True

def setup_lora_model(config):
    """è®¾ç½®LoRAæ¨¡å‹"""
    print("ğŸš€ åˆå§‹åŒ–GPT-2 + LoRAæ¨¡å‹...")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = GPT2LMHeadModel.from_pretrained(config.base_model)
    
    # é…ç½®LoRA
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,  # å› æœè¯­è¨€å»ºæ¨¡
        inference_mode=False,
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        lora_dropout=config.lora_dropout,
        target_modules=config.lora_target_modules,
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    model.print_trainable_parameters()
    
    return model

class CustomTrainer(Trainer):
    """è‡ªå®šä¹‰Trainerä»¥æ”¯æŒæ—©åœ"""
    
    def __init__(self, *args, early_stopping_callback=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.early_stopping_callback = early_stopping_callback
    
    def evaluation_loop(self, *args, **kwargs):
        """é‡å†™è¯„ä¼°å¾ªç¯ä»¥é›†æˆæ—©åœ"""
        output = super().evaluation_loop(*args, **kwargs)
        
        # è°ƒç”¨æ—©åœå›è°ƒ
        if self.early_stopping_callback:
            self.early_stopping_callback.on_evaluate(
                self.args,
                self.state,
                self.control,
                output.metrics
            )
        
        return output
    
    def training_step(self, *args, **kwargs):
        """é‡å†™è®­ç»ƒæ­¥éª¤ä»¥æ£€æŸ¥æ—©åœ"""
        output = super().training_step(*args, **kwargs)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
        if self.early_stopping_callback:
            self.early_stopping_callback.on_step_end(
                self.args,
                self.state,
                self.control
            )
        
        return output

def train_lora_gpt2(config):
    """è®­ç»ƒLoRAå¾®è°ƒçš„GPT-2æ¨¡å‹ï¼ˆå¸¦æ—©åœï¼‰"""
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ® - æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨æ•°æ®åŠ è½½å™¨ï¼Œä¸éœ€è¦é¢å¤–çš„collate_fn
    print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    train_loader, val_loader, test_loader, tokenizer = get_dataloaders(config)
    
    # è®¾ç½®æ¨¡å‹
    model = setup_lora_model(config)
    model = model.to(device)
    
    # æ—©åœå›è°ƒ
    early_stopping = EarlyStoppingCallback(
        patience=config.early_stopping_patience,
        min_delta=config.early_stopping_threshold,
        save_path="./gpt2-lora-best"
    )
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="./gpt2-lora-output",
        overwrite_output_dir=True,
        num_train_epochs=config.num_epochs,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        warmup_steps=config.warmup_steps,
        logging_steps=config.logging_steps,
        eval_steps=config.eval_steps,
        save_steps=config.save_steps,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,  # è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        learning_rate=config.learning_rate,
        weight_decay=0.01,
        fp16=torch.cuda.is_available(),
        dataloader_pin_memory=False,
        report_to=None,
        save_total_limit=2,  # åªä¿å­˜2ä¸ªæ£€æŸ¥ç‚¹ä»¥èŠ‚çœç©ºé—´
    )
    
    # å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨æ•°æ®åŠ è½½å™¨çš„æ•°æ®é›†ï¼Œä½†éœ€è¦é‡æ–°åŒ…è£…
    # å› ä¸ºTraineræœŸæœ›çš„æ˜¯Datasetå¯¹è±¡ï¼Œè€Œä¸æ˜¯DataLoader
    
    # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†åŒ…è£…å™¨
    class DataLoaderDataset(torch.utils.data.Dataset):
        def __init__(self, dataloader):
            self.dataloader = dataloader
            # é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
            self.data = []
            for batch in dataloader:
                for i in range(len(batch['input_ids'])):
                    self.data.append({
                        'input_ids': batch['input_ids'][i],
                        'attention_mask': batch['attention_mask'][i],
                        'labels': batch['labels'][i]
                    })
            
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            return self.data[idx]
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = DataLoaderDataset(train_loader)
    val_dataset = DataLoaderDataset(val_loader)
    
    # åˆ›å»ºè‡ªå®šä¹‰Trainer
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        early_stopping_callback=early_stopping,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ...")
    print(f"â° æ—©åœæœºåˆ¶: å®¹å¿ {config.early_stopping_patience} æ¬¡æ— æ”¹å–„è¯„ä¼°")
    
    try:
        trainer.train()
        
        # æ£€æŸ¥æ˜¯å¦å› æ—©åœè€Œç»“æŸ
        if early_stopping.early_stop:
            print("ğŸ è®­ç»ƒå› æ—©åœæœºåˆ¶è€Œç»“æŸ")
        else:
            print("ğŸ è®­ç»ƒæ­£å¸¸å®Œæˆ")
            
    except KeyboardInterrupt:
        print("âš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        raise
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_model("./gpt2-lora-final")
    
    # è¯„ä¼°æ¨¡å‹
    print("ğŸ“ˆ è¯„ä¼°æ¨¡å‹...")
    eval_results = trainer.evaluate()
    print(f"æœ€ç»ˆè¯„ä¼°ç»“æœ: {eval_results}")
    
    return model, tokenizer

def train_lora_gpt2_simple(config):
    """ç®€åŒ–çš„è®­ç»ƒå¾ªç¯ï¼Œé¿å…å¤æ‚çš„DatasetåŒ…è£…"""
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    train_loader, val_loader, test_loader, tokenizer = get_dataloaders(config)
    
    # è®¾ç½®æ¨¡å‹
    model = setup_lora_model(config)
    model = model.to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
    
    # è®­ç»ƒçŠ¶æ€
    best_val_loss = float('inf')
    patience_counter = 0
    global_step = 0
    
    print("ğŸ¯ å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ...")
    print(f"â° æ—©åœæœºåˆ¶: å®¹å¿ {config.early_stopping_patience} æ¬¡æ— æ”¹å–„è¯„ä¼°")
    
    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
    total_train_steps = len(train_loader) * config.num_epochs
    print(f"ğŸ“Š æ€»è®­ç»ƒæ­¥æ•°: {total_train_steps}")
    
    # åˆ›å»ºä¸»è¿›åº¦æ¡
    main_pbar = tqdm(total=total_train_steps, desc="æ€»ä½“è®­ç»ƒè¿›åº¦", position=0)
    
    for epoch in range(config.num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0
        train_steps = 0
        
        # åˆ›å»ºepochè¿›åº¦æ¡
        epoch_pbar = tqdm(total=len(train_loader), desc=f"Epoch {epoch+1}/{config.num_epochs}", position=1, leave=False)
        
        for batch_idx, batch in enumerate(train_loader):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {
                'input_ids': batch['input_ids'].to(device),
                'attention_mask': batch['attention_mask'].to(device),
                'labels': batch['labels'].to(device)
            }
            
            # å‰å‘ä¼ æ’­
            outputs = model(**inputs)
            loss = outputs.loss
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            total_train_loss += loss.item()
            train_steps += 1
            global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            current_loss = total_train_loss / train_steps
            epoch_pbar.set_postfix({
                'loss': f'{current_loss:.4f}',
                'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'
            })
            epoch_pbar.update(1)
            main_pbar.update(1)
            
            # è®°å½•æ—¥å¿—
            if global_step % config.logging_steps == 0:
                avg_loss = total_train_loss / train_steps
                print(f"\nğŸ“ Step {global_step}, Loss: {avg_loss:.4f}")
                total_train_loss = 0
                train_steps = 0
        
        epoch_pbar.close()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        total_val_loss = 0
        val_steps = 0
        
        print("ğŸ” éªŒè¯ä¸­...")
        # åˆ›å»ºéªŒè¯è¿›åº¦æ¡
        val_pbar = tqdm(total=len(val_loader), desc="éªŒè¯è¿›åº¦", position=1, leave=False)
        
        with torch.no_grad():
            for batch in val_loader:
                inputs = {
                    'input_ids': batch['input_ids'].to(device),
                    'attention_mask': batch['attention_mask'].to(device),
                    'labels': batch['labels'].to(device)
                }
                
                outputs = model(**inputs)
                total_val_loss += outputs.loss.item()
                val_steps += 1
                
                val_pbar.update(1)
        
        val_pbar.close()
        
        avg_val_loss = total_val_loss / val_steps
        print(f"ğŸ“Š Epoch {epoch+1}, éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss - config.early_stopping_threshold:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            model.save_pretrained("./gpt2-lora-best")
            tokenizer.save_pretrained("./gpt2-lora-best")
            print(f"ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        else:
            patience_counter += 1
            print(f"â³ æ—©åœè®¡æ•°: {patience_counter}/{config.early_stopping_patience}")
            
            if patience_counter >= config.early_stopping_patience:
                print("ğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶!")
                break
    
    # å…³é—­ä¸»è¿›åº¦æ¡
    main_pbar.close()
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    try:
        model = GPT2LMHeadModel.from_pretrained("./gpt2-lora-best")
        model = get_peft_model(model, LoraConfig.from_pretrained("./gpt2-lora-best"))
        model = model.to(device)
        print("ğŸ’¾ åŠ è½½æœ€ä½³æ¨¡å‹å®Œæˆ")
    except:
        print("âš ï¸ æ— æ³•åŠ è½½æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save_pretrained("./gpt2-lora-final")
    tokenizer.save_pretrained("./gpt2-lora-final")
    print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹å®Œæˆ")
    
    return model, tokenizer

def train_lora_gpt2_with_tqdm(config):
    """ä½¿ç”¨tqdmçš„å¢å¼ºç‰ˆè®­ç»ƒå¾ªç¯"""
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    train_loader, val_loader, test_loader, tokenizer = get_dataloaders(config)
    
    # è®¾ç½®æ¨¡å‹
    model = setup_lora_model(config)
    model = model.to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * config.num_epochs)
    
    # è®­ç»ƒçŠ¶æ€
    best_val_loss = float('inf')
    patience_counter = 0
    global_step = 0
    
    # è®­ç»ƒç»Ÿè®¡
    train_losses = []
    val_losses = []
    
    print("ğŸ¯ å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ...")
    print(f"â° æ—©åœæœºåˆ¶: å®¹å¿ {config.early_stopping_patience} æ¬¡æ— æ”¹å–„è¯„ä¼°")
    
    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
    total_train_steps = len(train_loader) * config.num_epochs
    print(f"ğŸ“Š æ€»è®­ç»ƒæ­¥æ•°: {total_train_steps}")
    
    # åˆ›å»ºä¸»è¿›åº¦æ¡
    main_pbar = tqdm(total=total_train_steps, desc="æ€»ä½“è®­ç»ƒè¿›åº¦", position=0)
    
    for epoch in range(config.num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_train_loss = 0
        train_batches = 0
        
        # åˆ›å»ºepochè¿›åº¦æ¡
        epoch_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.num_epochs}", position=1, leave=False)
        
        for batch in epoch_pbar:
            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {
                'input_ids': batch['input_ids'].to(device),
                'attention_mask': batch['attention_mask'].to(device),
                'labels': batch['labels'].to(device)
            }
            
            # å‰å‘ä¼ æ’­
            outputs = model(**inputs)
            loss = outputs.loss
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
            
            epoch_train_loss += loss.item()
            train_batches += 1
            global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            current_loss = epoch_train_loss / train_batches
            current_lr = scheduler.get_last_lr()[0]
            epoch_pbar.set_postfix({
                'loss': f'{current_loss:.4f}',
                'lr': f'{current_lr:.2e}'
            })
            main_pbar.update(1)
            
            # è®°å½•æ—¥å¿—
            if global_step % config.logging_steps == 0:
                print(f"\nğŸ“ Step {global_step}, Loss: {current_loss:.4f}, LR: {current_lr:.2e}")
        
        epoch_pbar.close()
        avg_train_loss = epoch_train_loss / train_batches
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        epoch_val_loss = 0
        val_batches = 0
        
        print("ğŸ” éªŒè¯ä¸­...")
        # åˆ›å»ºéªŒè¯è¿›åº¦æ¡
        val_pbar = tqdm(val_loader, desc="éªŒè¯è¿›åº¦", position=1, leave=False)
        
        with torch.no_grad():
            for batch in val_pbar:
                inputs = {
                    'input_ids': batch['input_ids'].to(device),
                    'attention_mask': batch['attention_mask'].to(device),
                    'labels': batch['labels'].to(device)
                }
                
                outputs = model(**inputs)
                epoch_val_loss += outputs.loss.item()
                val_batches += 1
                
                # æ›´æ–°éªŒè¯è¿›åº¦æ¡
                current_val_loss = epoch_val_loss / val_batches
                val_pbar.set_postfix({'val_loss': f'{current_val_loss:.4f}'})
        
        val_pbar.close()
        
        avg_val_loss = epoch_val_loss / val_batches
        val_losses.append(avg_val_loss)
        
        print(f"ğŸ“Š Epoch {epoch+1} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        print(f"  å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss - config.early_stopping_threshold:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            model.save_pretrained("./gpt2-lora-best")
            tokenizer.save_pretrained("./gpt2-lora-best")
            print(f"ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        else:
            patience_counter += 1
            print(f"â³ æ—©åœè®¡æ•°: {patience_counter}/{config.early_stopping_patience}")
            
            if patience_counter >= config.early_stopping_patience:
                print("ğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶!")
                break
    
    # å…³é—­ä¸»è¿›åº¦æ¡
    main_pbar.close()
    
    # æ‰“å°è®­ç»ƒæ€»ç»“
    print("\nğŸ“ˆ è®­ç»ƒæ€»ç»“:")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
    print(f"  è®­ç»ƒè½®æ¬¡: {len(train_losses)}")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    try:
        model = GPT2LMHeadModel.from_pretrained("./gpt2-lora-best")
        model = get_peft_model(model, LoraConfig.from_pretrained("./gpt2-lora-best"))
        model = model.to(device)
        print("ğŸ’¾ åŠ è½½æœ€ä½³æ¨¡å‹å®Œæˆ")
    except:
        print("âš ï¸ æ— æ³•åŠ è½½æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save_pretrained("./gpt2-lora-final")
    tokenizer.save_pretrained("./gpt2-lora-final")
    print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹å®Œæˆ")
    
    return model, tokenizer

def test_generation(model, tokenizer, device):
    """æµ‹è¯•ç”Ÿæˆæ•ˆæœ"""
    print("\nğŸ§ª æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
    
    # æµ‹è¯•æç¤º
    test_prompts = [
        "Instruction: Explain the concept of machine learning in simple terms.\nResponse:",
        "Question: What is the capital of France?\nAnswer:",
        "Math Problem: If a train travels at 60 mph for 2 hours, how far does it go?\nSolution:"
    ]
    
    model.eval()
    
    for i, prompt in enumerate(test_prompts):
        print(f"\n--- æµ‹è¯• {i+1} ---")
        print(f"æç¤º: {prompt}")
        
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=len(inputs['input_ids'][0]) + 100,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=2
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ç”Ÿæˆç»“æœ: {generated_text}")

def main():
    """ä¸»å‡½æ•°"""
    config = LoRAConfig()
    
    # é€‰æ‹©è®­ç»ƒæ–¹æ³•
    training_method = "enhanced"  # å¯é€‰: "simple", "enhanced"
    
    if training_method == "simple":
        # ä½¿ç”¨ç®€åŒ–è®­ç»ƒå¾ªç¯
        model, tokenizer = train_lora_gpt2_simple(config)
    elif training_method == "enhanced":
        # ä½¿ç”¨å¢å¼ºç‰ˆè®­ç»ƒå¾ªç¯ï¼ˆæ¨èï¼‰
        model, tokenizer = train_lora_gpt2_with_tqdm(config)
    else:
        # ä½¿ç”¨Trainerç‰ˆæœ¬
        model, tokenizer = train_lora_gpt2(config)
    
    # æµ‹è¯•ç”Ÿæˆ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    test_generation(model, tokenizer, device)
    
    print("âœ… LoRAå¾®è°ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()