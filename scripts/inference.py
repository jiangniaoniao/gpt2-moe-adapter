import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import torch
from transformers import GPT2Tokenizer

from config.base_config import GPT2MoEConfig, MoEAdapterConfig
from models.gpt2_moe_model import GPT2WithMoEAdapter

def generate_text():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
    
    # åŠ è½½é…ç½®
    adapter_config = MoEAdapterConfig(
        num_experts=8,
        expert_size=512,
        router_type="soft"
    )
    
    # ä½¿ç”¨æœ¬åœ°GPT-2æƒé‡è·¯å¾„
    local_model_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "gpt2")
    
    model_config = GPT2MoEConfig(
        base_model=local_model_path,  # ä½¿ç”¨æœ¬åœ°è·¯å¾„
        num_adapter_layers=6,
        adapter_layers=[2, 4, 6, 8, 10, 12],
        freeze_base_model=True,
        adapter_config=adapter_config
    )
    
    # åˆ›å»ºæ¨¡å‹å’Œtokenizer
    model = GPT2WithMoEAdapter(model_config)
    tokenizer = GPT2Tokenizer.from_pretrained(local_model_path)
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½è®­ç»ƒå¥½çš„é€‚é…å™¨æƒé‡
    adapter_weights = torch.load("./output/best_model_epoch_0/adapter_weights.pth")
    model.load_state_dict(adapter_weights, strict=False)
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    
    # ç”Ÿæˆæ–‡æœ¬
    prompt = "The future of artificial intelligence is"
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_length=100,
            num_return_sequences=1,
            temperature=0.8,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    print(f"ğŸ¤– ç”Ÿæˆçš„æ–‡æœ¬:")
    print(f"{generated_text}")

if __name__ == "__main__":
    generate_text()