import torch
import torch.nn as nn
from transformers import GPT2LMHeadModel, GPT2Config
import os

from .moe_adapter import MoEAdapterLayer

class GPT2WithMoEAdapter(nn.Module):
    """é›†æˆMoE Adapterçš„GPT-2æ¨¡å‹"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        adapter_config = config.adapter_config
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
        if os.path.exists(config.base_model):
            # åŠ è½½æœ¬åœ°GPT-2æ¨¡å‹
            print(f"ğŸ“ ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {config.base_model}")
            self.gpt2 = GPT2LMHeadModel.from_pretrained(config.base_model)
        else:
            # ä»HuggingFaceåŠ è½½æ¨¡å‹
            print(f"ğŸŒ ä»HuggingFaceåŠ è½½æ¨¡å‹: {config.base_model}")
            self.gpt2 = GPT2LMHeadModel.from_pretrained(config.base_model)
        
        gpt2_config = self.gpt2.config
        
        # å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°
        if config.freeze_base_model:
            for param in self.gpt2.parameters():
                param.requires_grad = False
        
        # åˆ›å»ºMoE Adapterå±‚
        self.moe_adapters = nn.ModuleList([
            MoEAdapterLayer(adapter_config) for _ in range(len(config.adapter_layers))
        ])
        
        print(f"âœ… åˆå§‹åŒ–GPT-2 + MoE Adapteræ¨¡å‹")
        print(f"   - åŸºç¡€æ¨¡å‹: {config.base_model}")
        print(f"   - é€‚é…å™¨å±‚: {config.adapter_layers}")
        print(f"   - ä¸“å®¶æ•°é‡: {adapter_config.num_experts}")
        print(f"   - è·¯ç”±å™¨ç±»å‹: {adapter_config.router_type}")
        
    def forward(self, input_ids, attention_mask=None, labels=None):
        """
        å‚æ•°:
            input_ids: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]
            labels: [batch_size, seq_len]
        è¿”å›:
            dict: åŒ…å«æŸå¤±ã€logitså’Œè·¯ç”±å™¨ç»Ÿè®¡ä¿¡æ¯
        """
        # GPT-2å‰å‘ä¼ æ’­ï¼Œè·å–æ‰€æœ‰éšè—çŠ¶æ€
        outputs = self.gpt2.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True
        )
        
        hidden_states = outputs.hidden_states  # æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€
        
        # åº”ç”¨MoE Adapter
        adapter_outputs = []
        total_load_loss = torch.tensor(0.0, device=input_ids.device)
        router_metrics_list = []
        
        current_adapter_idx = 0
        for layer_idx, hidden_state in enumerate(hidden_states):
            if layer_idx in self.config.adapter_layers and current_adapter_idx < len(self.moe_adapters):
                # åº”ç”¨MoE Adapter
                adapted_output, router_metrics = self.moe_adapters[current_adapter_idx](hidden_state)
                adapter_outputs.append(adapted_output)
                
                # ç´¯åŠ è´Ÿè½½å‡è¡¡æŸå¤±
                load_loss = self.moe_adapters[current_adapter_idx].load_balancing_loss(router_metrics)
                total_load_loss = total_load_loss + load_loss
                
                # æ”¶é›†è·¯ç”±å™¨ç»Ÿè®¡ä¿¡æ¯
                router_metrics_list.append({
                    'layer': layer_idx,
                    'expert_usage': router_metrics['expert_usage']
                })
                
                current_adapter_idx += 1
            else:
                adapter_outputs.append(hidden_state)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªéšè—çŠ¶æ€
        last_hidden_state = adapter_outputs[-1]
        
        # é€šè¿‡GPT-2çš„LMå¤´è®¡ç®—logits
        lm_logits = self.gpt2.lm_head(last_hidden_state)
        
        # è®¡ç®—æŸå¤±
        total_loss = None
        lm_loss = None
        
        if labels is not None:
            # ç§»ä½logitså’Œlabelsç”¨äºè¯­è¨€å»ºæ¨¡
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            
            loss_fct = nn.CrossEntropyLoss()
            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            total_loss = lm_loss + total_load_loss
        
        return {
            'loss': total_loss,
            'logits': lm_logits,
            'lm_loss': lm_loss,
            'load_balancing_loss': total_load_loss,
            'hidden_states': adapter_outputs,
            'router_metrics': router_metrics_list
        }
    
    def generate(self, input_ids, **kwargs):
        """ç”Ÿæˆæ–‡æœ¬ - ç›´æ¥ä½¿ç”¨åŸºç¡€GPT-2çš„ç”Ÿæˆæ–¹æ³•"""
        return self.gpt2.generate(input_ids, **kwargs)